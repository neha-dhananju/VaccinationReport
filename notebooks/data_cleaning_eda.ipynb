{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2d2d946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Converted coverage-data.xlsx ‚Üí coverage_data.csv\n",
      "‚úÖ Converted incidence-rate-data.xlsx ‚Üí incidence_rate.csv\n",
      "‚úÖ Converted reported-cases-data.xlsx ‚Üí reported_cases.csv\n",
      "‚úÖ Converted vaccine-introduction-data.xlsx ‚Üí vaccine_intro.csv\n",
      "‚úÖ Converted vaccine-schedule-data.xlsx ‚Üí vaccine_schedule.csv\n",
      "\n",
      " All Excel files converted to CSV successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "data_path = \"../data\"\n",
    "output_path = \"../cleaned\"\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Mapping of xlsx files to csv filenames\n",
    "files = {\n",
    "    \"coverage-data.xlsx\": \"coverage_data.csv\",\n",
    "    \"incidence-rate-data.xlsx\": \"incidence_rate.csv\",\n",
    "    \"reported-cases-data.xlsx\": \"reported_cases.csv\",\n",
    "    \"vaccine-introduction-data.xlsx\": \"vaccine_intro.csv\",\n",
    "    \"vaccine-schedule-data.xlsx\": \"vaccine_schedule.csv\"\n",
    "}\n",
    "\n",
    "# Convert all XLSX files to CSV\n",
    "for xlsx_file, csv_file in files.items():\n",
    "    file_path = os.path.join(data_path, xlsx_file)\n",
    "    df = pd.read_excel(file_path, engine=\"openpyxl\")\n",
    "    df.to_csv(os.path.join(output_path, csv_file), index=False)\n",
    "    print(f\"‚úÖ Converted {xlsx_file} ‚Üí {csv_file}\")\n",
    "\n",
    "print(\"\\n All Excel files converted to CSV successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "10cffa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "\n",
    "# Visualization settings\n",
    "sns.set(style=\"whitegrid\")\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a3535a",
   "metadata": {},
   "source": [
    "## Coverage Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bdb536b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset Loaded Successfully!\n",
      "(399859, 11)\n",
      "       GROUP CODE   NAME    YEAR  ANTIGEN                                ANTIGEN_DESCRIPTION COVERAGE_CATEGORY COVERAGE_CATEGORY_DESCRIPTION  TARGET_NUMBER   DOSES  COVERAGE\n",
      "0  COUNTRIES  ABW  Aruba  2023.0      BCG                                                BCG             ADMIN       Administrative coverage            NaN     NaN       NaN\n",
      "1  COUNTRIES  ABW  Aruba  2023.0      BCG                                                BCG          OFFICIAL             Official coverage            NaN     NaN       NaN\n",
      "2  COUNTRIES  ABW  Aruba  2023.0  DIPHCV4  Diphtheria-containing vaccine, 4th dose (1st b...             ADMIN       Administrative coverage         1044.0   945.0     90.52\n",
      "3  COUNTRIES  ABW  Aruba  2023.0  DIPHCV4  Diphtheria-containing vaccine, 4th dose (1st b...          OFFICIAL             Official coverage            NaN     NaN     90.52\n",
      "4  COUNTRIES  ABW  Aruba  2023.0  DIPHCV5  Diphtheria-containing vaccine, 5th dose (2nd b...             ADMIN       Administrative coverage         1219.0  1008.0     82.69\n",
      "\n",
      "Missing Values Before Handling:\n",
      " GROUP                                 0\n",
      "CODE                                  1\n",
      "NAME                               1275\n",
      "YEAR                                  1\n",
      "ANTIGEN                               1\n",
      "ANTIGEN_DESCRIPTION                   1\n",
      "COVERAGE_CATEGORY                     1\n",
      "COVERAGE_CATEGORY_DESCRIPTION         1\n",
      "TARGET_NUMBER                    320829\n",
      "DOSES                            320532\n",
      "COVERAGE                         169382\n",
      "dtype: int64\n",
      "\n",
      "Missing Values After Handling:\n",
      " GROUP                            0\n",
      "CODE                             0\n",
      "NAME                             0\n",
      "YEAR                             0\n",
      "ANTIGEN                          0\n",
      "ANTIGEN_DESCRIPTION              0\n",
      "COVERAGE_CATEGORY                0\n",
      "COVERAGE_CATEGORY_DESCRIPTION    0\n",
      "TARGET_NUMBER                    0\n",
      "DOSES                            0\n",
      "COVERAGE                         0\n",
      "dtype: int64\n",
      "\n",
      "Total Duplicate Rows: 0\n",
      "Rows with Negative Values: 8\n",
      "\n",
      "‚úÖ Final Dataset Overview:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 399850 entries, 0 to 399857\n",
      "Data columns (total 15 columns):\n",
      " #   Column                         Non-Null Count   Dtype  \n",
      "---  ------                         --------------   -----  \n",
      " 0   GROUP                          399850 non-null  object \n",
      " 1   CODE                           399850 non-null  object \n",
      " 2   NAME                           399850 non-null  object \n",
      " 3   YEAR                           399850 non-null  float64\n",
      " 4   ANTIGEN                        399850 non-null  object \n",
      " 5   ANTIGEN_DESCRIPTION            399850 non-null  object \n",
      " 6   COVERAGE_CATEGORY              399850 non-null  object \n",
      " 7   COVERAGE_CATEGORY_DESCRIPTION  399850 non-null  object \n",
      " 8   TARGET_NUMBER                  399850 non-null  float64\n",
      " 9   DOSES                          399850 non-null  float64\n",
      " 10  COVERAGE                       399850 non-null  float64\n",
      " 11  COVERAGE_STATUS                399850 non-null  object \n",
      " 12  DOSE_GAP                       399850 non-null  float64\n",
      " 13  VACCINE_EFFICIENCY             399850 non-null  float64\n",
      " 14  RECENT_DATA                    399850 non-null  object \n",
      "dtypes: float64(6), object(9)\n",
      "memory usage: 56.9+ MB\n",
      "None\n",
      "                YEAR  TARGET_NUMBER         DOSES       COVERAGE      DOSE_GAP  VACCINE_EFFICIENCY\n",
      "count  399850.000000   3.998500e+05  3.998500e+05  399850.000000  3.998500e+05       399850.000000\n",
      "mean     2009.207238   5.538809e+07  8.464693e+05      75.436394  5.454162e+07           62.469741\n",
      "std        11.720510   2.407509e+10  5.167779e+06      25.970341  2.407508e+10           28.459849\n",
      "min      1980.000000   0.000000e+00  0.000000e+00       0.000000  0.000000e+00            0.000000\n",
      "25%      2002.000000   1.950530e+05  7.940500e+04      64.000000  3.591150e+04           47.973574\n",
      "50%      2012.000000   3.511920e+05  1.802580e+05      87.000000  6.051300e+04           69.742138\n",
      "75%      2019.000000   4.612320e+05  3.086895e+05      93.000000  1.731895e+05           85.819567\n",
      "max      2023.000000   1.170000e+13  1.266052e+08     100.000000  1.169999e+13          100.000000\n",
      "\n",
      " Data Cleaning & Feature Engineering Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------------------------------------\n",
    "# STEP 2: Load Dataset\n",
    "# ----------------------------------------------\n",
    "data_path = \"../cleaned\"\n",
    "df_coverage = pd.read_csv(os.path.join(data_path, \"coverage_data.csv\"))\n",
    "\n",
    "print(\"‚úÖ Dataset Loaded Successfully!\")\n",
    "print(df_coverage.shape)\n",
    "print(df_coverage.head())\n",
    "\n",
    "# ----------------------------------------------\n",
    "# STEP 3: Handle Missing Values\n",
    "# ----------------------------------------------\n",
    "\n",
    "# 3.1 Check missing values\n",
    "print(\"\\nMissing Values Before Handling:\\n\", df_coverage.isna().sum())\n",
    "\n",
    "# 3.2 Fix COUNTRY NAME & CODE Mapping\n",
    "\n",
    "# We know 'WB_LONG_NA' and 'WB_SHORT_NA' are aggregate regions, not countries.\n",
    "# Option ‚Üí Assign a placeholder name instead of dropping\n",
    "df_coverage.loc[df_coverage['CODE'] == \"WB_LONG_NA\", \"NAME\"] = \"World Bank Long-term Aggregate\"\n",
    "df_coverage.loc[df_coverage['CODE'] == \"WB_SHORT_NA\", \"NAME\"] = \"World Bank Short-term Aggregate\"\n",
    "\n",
    "# Drop rows where CODE is missing (only 1 row)\n",
    "df_coverage = df_coverage.dropna(subset=['CODE'])\n",
    "\n",
    "\n",
    "# 3.3 Handle Remaining Missing Values\n",
    "\n",
    "# Fill COVERAGE with median per antigen\n",
    "df_coverage['COVERAGE'] = df_coverage.groupby('ANTIGEN')['COVERAGE'].transform(\n",
    "    lambda x: x.fillna(x.median())\n",
    ")\n",
    "\n",
    "# Fill DOSES with median per antigen\n",
    "df_coverage['DOSES'] = df_coverage.groupby('ANTIGEN')['DOSES'].transform(\n",
    "    lambda x: x.fillna(x.median())\n",
    ")\n",
    "\n",
    "# Fill TARGET_NUMBER with median per antigen\n",
    "df_coverage['TARGET_NUMBER'] = df_coverage.groupby('ANTIGEN')['TARGET_NUMBER'].transform(\n",
    "    lambda x: x.fillna(x.median())\n",
    ")\n",
    "\n",
    "# Drop rows where COVERAGE is still missing after imputation\n",
    "df_coverage = df_coverage.dropna(subset=['COVERAGE'])\n",
    "\n",
    "print(\"\\nMissing Values After Handling:\\n\", df_coverage.isna().sum())\n",
    "\n",
    "# ----------------------------------------------\n",
    "# STEP 4: Normalize Units (if required)\n",
    "# ----------------------------------------------\n",
    "# COVERAGE should always be between 0 and 100 (percentage)\n",
    "df_coverage['COVERAGE'] = df_coverage['COVERAGE'].clip(0, 100)\n",
    "\n",
    "# ----------------------------------------------\n",
    "# STEP 5: Consistency Checks\n",
    "# ----------------------------------------------\n",
    "\n",
    "# 5.1 Check Duplicates\n",
    "duplicates = df_coverage[df_coverage.duplicated()]\n",
    "print(f\"\\nTotal Duplicate Rows: {duplicates.shape[0]}\")\n",
    "df_coverage.drop_duplicates(inplace=True)\n",
    "\n",
    "# 5.2 Check Negative Values for DOSES & TARGET_NUMBER\n",
    "negatives = df_coverage[(df_coverage['DOSES'] < 0) | (df_coverage['TARGET_NUMBER'] < 0)]\n",
    "print(f\"Rows with Negative Values: {negatives.shape[0]}\")\n",
    "df_coverage = df_coverage[~((df_coverage['DOSES'] < 0) | (df_coverage['TARGET_NUMBER'] < 0))]\n",
    "\n",
    "# 5.3 Fix DOSES > TARGET_NUMBER (Cap doses at target)\n",
    "df_coverage.loc[df_coverage['DOSES'] > df_coverage['TARGET_NUMBER'], 'DOSES'] = df_coverage['TARGET_NUMBER']\n",
    "\n",
    "# 5.4 Fix COVERAGE > 100\n",
    "df_coverage.loc[df_coverage['COVERAGE'] > 100, 'COVERAGE'] = 100\n",
    "\n",
    "# 5.5 Validate Year Range\n",
    "df_coverage = df_coverage[(df_coverage['YEAR'] >= 1980) & (df_coverage['YEAR'] <= 2025)]\n",
    "\n",
    "# ----------------------------------------------\n",
    "# STEP 6: Feature Engineering (FE)\n",
    "# ----------------------------------------------\n",
    "\n",
    "# 6.1 Coverage Category - Binary Feature\n",
    "def categorize_coverage(value):\n",
    "    if value >= 90:\n",
    "        return \"High\"\n",
    "    elif value >= 70:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Low\"\n",
    "\n",
    "df_coverage['COVERAGE_STATUS'] = df_coverage['COVERAGE'].apply(categorize_coverage)\n",
    "\n",
    "\n",
    "# 6.2 Calculate Dose Gap\n",
    "df_coverage['DOSE_GAP'] = df_coverage['TARGET_NUMBER'] - df_coverage['DOSES']\n",
    "\n",
    "# 6.3 Vaccination Efficiency (DOSES / TARGET_NUMBER)\n",
    "df_coverage['VACCINE_EFFICIENCY'] = np.where(\n",
    "    df_coverage['TARGET_NUMBER'] > 0,\n",
    "    (df_coverage['DOSES'] / df_coverage['TARGET_NUMBER']) * 100,\n",
    "    0\n",
    ")\n",
    "\n",
    "# 6.4 Year-wise Coverage Flag\n",
    "df_coverage['RECENT_DATA'] = df_coverage['YEAR'].apply(lambda x: 'Recent' if x >= 2018 else 'Old')\n",
    "\n",
    "# ----------------------------------------------\n",
    "# STEP 7: Final Sanity Check\n",
    "# ----------------------------------------------\n",
    "print(\"\\n‚úÖ Final Dataset Overview:\\n\")\n",
    "print(df_coverage.info())\n",
    "print(df_coverage.describe())\n",
    "\n",
    "# ----------------------------------------------\n",
    "# STEP 8: Save Cleaned Dataset\n",
    "# ----------------------------------------------\n",
    "cleaned_path = \"../cleaned\"\n",
    "os.makedirs(cleaned_path, exist_ok=True)\n",
    "\n",
    "df_coverage.to_csv(os.path.join(cleaned_path, \"coverage_data_cleaned.csv\"), index=False)\n",
    "print(\"\\n Data Cleaning & Feature Engineering Completed Successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbb48e4",
   "metadata": {},
   "source": [
    "## Incidence Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8a3d6962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Incidence Dataset Loaded!\n",
      "(84946, 8)\n",
      "       GROUP CODE   NAME    YEAR          DISEASE             DISEASE_DESCRIPTION                     DENOMINATOR  INCIDENCE_RATE\n",
      "0  COUNTRIES  ABW  Aruba  2023.0              CRS     Congenital rubella syndrome          per 10,000 live births             0.0\n",
      "1  COUNTRIES  ABW  Aruba  2023.0       DIPHTHERIA                      Diphtheria  per 1,000,000 total population             0.0\n",
      "2  COUNTRIES  ABW  Aruba  2023.0  INVASIVE_MENING  Invasive meningococcal disease  per 1,000,000 total population             9.3\n",
      "3  COUNTRIES  ABW  Aruba  2023.0          MEASLES                         Measles  per 1,000,000 total population             NaN\n",
      "4  COUNTRIES  ABW  Aruba  2023.0            MUMPS                           Mumps  per 1,000,000 total population             0.0\n",
      "\n",
      "Missing Values Before Handling:\n",
      " GROUP                      0\n",
      "CODE                       1\n",
      "NAME                       1\n",
      "YEAR                       1\n",
      "DISEASE                    1\n",
      "DISEASE_DESCRIPTION        1\n",
      "DENOMINATOR                1\n",
      "INCIDENCE_RATE         23362\n",
      "dtype: int64\n",
      "['per 10,000 live births' 'per 1,000,000 total population'\n",
      " 'per 1,000 live births' 'per 1,000,000 <15 population'\n",
      " 'per 1000 live births']\n",
      "                      DENOMINATOR  DENOMINATOR_VALUE\n",
      "0          per 10,000 live births              10000\n",
      "1  per 1,000,000 total population            1000000\n",
      "2  per 1,000,000 total population            1000000\n",
      "3  per 1,000,000 total population            1000000\n",
      "4  per 1,000,000 total population            1000000\n",
      "\n",
      "Missing Values After Handling:\n",
      " GROUP                  0\n",
      "CODE                   0\n",
      "NAME                   0\n",
      "YEAR                   0\n",
      "DISEASE                0\n",
      "DISEASE_DESCRIPTION    0\n",
      "DENOMINATOR            0\n",
      "INCIDENCE_RATE         0\n",
      "DENOMINATOR_VALUE      0\n",
      "dtype: int64\n",
      "\n",
      "Total Duplicate Rows: 0\n",
      "\n",
      "‚úÖ Final Dataset Overview:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 84945 entries, 0 to 84944\n",
      "Data columns (total 11 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   GROUP                84945 non-null  object \n",
      " 1   CODE                 84945 non-null  object \n",
      " 2   NAME                 84945 non-null  object \n",
      " 3   YEAR                 84945 non-null  float64\n",
      " 4   DISEASE              84945 non-null  object \n",
      " 5   DISEASE_DESCRIPTION  84945 non-null  object \n",
      " 6   DENOMINATOR          84945 non-null  object \n",
      " 7   INCIDENCE_RATE       84945 non-null  float64\n",
      " 8   DENOMINATOR_VALUE    84945 non-null  float64\n",
      " 9   INCIDENCE_SEVERITY   84945 non-null  object \n",
      " 10  RECENT_DATA          84945 non-null  object \n",
      "dtypes: float64(3), object(8)\n",
      "memory usage: 7.8+ MB\n",
      "None\n",
      "               YEAR  INCIDENCE_RATE  DENOMINATOR_VALUE\n",
      "count  84945.000000    84945.000000       84945.000000\n",
      "mean    2004.095791       79.828795      831958.467244\n",
      "std       12.595166      846.253958      372956.655119\n",
      "min     1980.000000        0.000000        1000.000000\n",
      "25%     1994.000000        0.000000     1000000.000000\n",
      "50%     2005.000000        0.000000     1000000.000000\n",
      "75%     2015.000000        2.800000     1000000.000000\n",
      "max     2023.000000    69101.300000     1000000.000000\n",
      "\n",
      " Incidence Rate Data Cleaning & Feature Engineering Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 2: Load Dataset\n",
    "# ---------------------------------------------------------\n",
    "data_path = \"../cleaned\"\n",
    "df_incidence = pd.read_csv(os.path.join(data_path, \"incidence_rate.csv\"))\n",
    "\n",
    "print(\"‚úÖ Incidence Dataset Loaded!\")\n",
    "print(df_incidence.shape)\n",
    "print(df_incidence.head())\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 3: Handle Missing Values\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 3.1 Check missing values\n",
    "print(\"\\nMissing Values Before Handling:\\n\", df_incidence.isna().sum())\n",
    "\n",
    "# 3.2 Handle Missing CODE and NAME\n",
    "\n",
    "# Drop rows where CODE is missing (critical identifier)\n",
    "df_incidence = df_incidence.dropna(subset=['CODE'])\n",
    "\n",
    "# Build CODE ‚Üí NAME mapping from existing valid values\n",
    "code_to_name = (\n",
    "    df_incidence.dropna(subset=['NAME'])\n",
    "               .drop_duplicates(subset=['CODE'], keep='first')\n",
    "               .set_index('CODE')['NAME']\n",
    "               .to_dict()\n",
    ")\n",
    "\n",
    "# Fill missing NAME using CODE mapping\n",
    "df_incidence['NAME'] = df_incidence['NAME'].fillna(df_incidence['CODE'].map(code_to_name))\n",
    "\n",
    "# Handle World Bank aggregates (if exist)\n",
    "wb_map = {\n",
    "    'WB_LONG_NA': 'World Bank Long-term Aggregate',\n",
    "    'WB_SHORT_NA': 'World Bank Short-term Aggregate'\n",
    "}\n",
    "df_incidence.loc[df_incidence['CODE'].isin(wb_map.keys()), 'NAME'] = \\\n",
    "    df_incidence.loc[df_incidence['CODE'].isin(wb_map.keys()), 'CODE'].map(wb_map)\n",
    "\n",
    "# Final fallback: fill missing NAME with \"Unknown\"\n",
    "df_incidence['NAME'] = df_incidence['NAME'].fillna('Unknown')\n",
    "\n",
    "# 3.3 Handle Missing Disease Descriptions\n",
    "df_incidence['DISEASE_DESCRIPTION'] = df_incidence['DISEASE_DESCRIPTION'].fillna(\n",
    "    df_incidence['DISEASE']\n",
    ")\n",
    "\n",
    "# 3.4 Handle Missing Denominator (population basis)\n",
    "# Fill with median per disease\n",
    "# Strip spaces and lowercase for consistency\n",
    "df_incidence['DENOMINATOR'] = df_incidence['DENOMINATOR'].str.strip().str.lower()\n",
    "\n",
    "# Check unique values\n",
    "print(df_incidence['DENOMINATOR'].unique())\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_number(text):\n",
    "    match = re.search(r\"(\\d[\\d,]*)\", str(text))\n",
    "    return int(match.group(1).replace(\",\", \"\")) if match else np.nan\n",
    "\n",
    "df_incidence['DENOMINATOR_VALUE'] = df_incidence['DENOMINATOR'].apply(extract_number)\n",
    "\n",
    "print(df_incidence[['DENOMINATOR', 'DENOMINATOR_VALUE']].head())\n",
    "\n",
    "# Fill missing denominator descriptions with \"per total population\" by default\n",
    "df_incidence['DENOMINATOR'] = df_incidence['DENOMINATOR'].fillna(\"per total population\")\n",
    "\n",
    "\n",
    "# 3.5 Handle Missing Incidence Rate\n",
    "# If INCIDENCE_RATE missing, calculate if CASES & DENOMINATOR exist\n",
    "if 'CASES' in df_incidence.columns:\n",
    "    df_incidence['INCIDENCE_RATE'] = np.where(\n",
    "        (df_incidence['DENOMINATOR'] > 0) & df_incidence['INCIDENCE_RATE'].isna(),\n",
    "        (df_incidence['CASES'] / df_incidence['DENOMINATOR_VALUE']) * 100000\n",
    ",\n",
    "        df_incidence['INCIDENCE_RATE']\n",
    "    )\n",
    "# Fill remaining missing values with median per disease\n",
    "df_incidence['INCIDENCE_RATE'] = df_incidence.groupby('DISEASE')['INCIDENCE_RATE'].transform(\n",
    "    lambda x: x.fillna(x.median())\n",
    ")\n",
    "\n",
    "print(\"\\nMissing Values After Handling:\\n\", df_incidence.isna().sum())\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 4: Normalize Units\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Ensure incidence rate ‚â• 0\n",
    "df_incidence.loc[df_incidence['INCIDENCE_RATE'] < 0, 'INCIDENCE_RATE'] = 0\n",
    "\n",
    "# Ensure denominator numeric values ‚â• 0\n",
    "df_incidence.loc[df_incidence['DENOMINATOR_VALUE'] < 0, 'DENOMINATOR_VALUE'] = np.nan\n",
    "\n",
    "# Fill missing numeric denominators with median per disease\n",
    "df_incidence['DENOMINATOR_VALUE'] = df_incidence.groupby('DISEASE')['DENOMINATOR_VALUE'].transform(\n",
    "    lambda x: x.fillna(x.median())\n",
    ")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 5: Consistency Checks\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 5.1 Remove duplicates\n",
    "print(f\"\\nTotal Duplicate Rows: {df_incidence.duplicated().sum()}\")\n",
    "df_incidence.drop_duplicates(inplace=True)\n",
    "\n",
    "# 5.2 Validate year range\n",
    "df_incidence = df_incidence[(df_incidence['YEAR'] >= 1980) & (df_incidence['YEAR'] <= 2025)]\n",
    "\n",
    "# 5.3 Ensure correct disease codes\n",
    "df_incidence['DISEASE'] = df_incidence['DISEASE'].str.upper().str.strip()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 6: Feature Engineering (FE)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 6.1 Create \"Incidence Severity\" based on thresholds\n",
    "def categorize_incidence(rate):\n",
    "    if rate == 0:\n",
    "        return \"No Cases\"\n",
    "    elif rate <= 10:\n",
    "        return \"Low\"\n",
    "    elif rate <= 50:\n",
    "        return \"Moderate\"\n",
    "    else:\n",
    "        return \"High\"\n",
    "\n",
    "df_incidence['INCIDENCE_SEVERITY'] = df_incidence['INCIDENCE_RATE'].apply(categorize_incidence)\n",
    "\n",
    "# 6.2 Create WHO Region if not present (optional ‚Äî depends on your dataset)\n",
    "# If there's a WHO region column, skip this. Otherwise, you can merge from another table later.\n",
    "\n",
    "# 6.3 Create \"Recent Data\" flag\n",
    "df_incidence['RECENT_DATA'] = df_incidence['YEAR'].apply(lambda x: 'Recent' if x >= 2018 else 'Old')\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 7: Final Sanity Check\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n‚úÖ Final Dataset Overview:\\n\")\n",
    "print(df_incidence.info())\n",
    "print(df_incidence.describe())\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 8: Save Cleaned Dataset\n",
    "# ---------------------------------------------------------\n",
    "cleaned_path = \"../cleaned\"\n",
    "os.makedirs(cleaned_path, exist_ok=True)\n",
    "\n",
    "df_incidence.to_csv(os.path.join(cleaned_path, \"incidence_rate_cleaned.csv\"), index=False)\n",
    "print(\"\\n Incidence Rate Data Cleaning & Feature Engineering Completed Successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4826406",
   "metadata": {},
   "source": [
    "## Reported Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d5da8069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Reported Cases Dataset Loaded!\n",
      "(84870, 7)\n",
      "       GROUP CODE   NAME    YEAR          DISEASE             DISEASE_DESCRIPTION  CASES\n",
      "0  COUNTRIES  ABW  Aruba  2023.0              CRS     Congenital rubella syndrome    0.0\n",
      "1  COUNTRIES  ABW  Aruba  2023.0       DIPHTHERIA                      Diphtheria    0.0\n",
      "2  COUNTRIES  ABW  Aruba  2023.0  INVASIVE_MENING  Invasive meningococcal disease    1.0\n",
      "3  COUNTRIES  ABW  Aruba  2023.0          MEASLES                         Measles    NaN\n",
      "4  COUNTRIES  ABW  Aruba  2023.0            MUMPS                           Mumps    0.0\n",
      "\n",
      "Missing Values Before Handling:\n",
      " GROUP                      0\n",
      "CODE                       1\n",
      "NAME                       1\n",
      "YEAR                       1\n",
      "DISEASE                    1\n",
      "DISEASE_DESCRIPTION        1\n",
      "CASES                  19400\n",
      "dtype: int64\n",
      "\n",
      "Total Duplicate Rows: 0\n",
      "\n",
      "‚úÖ Final Dataset Overview:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 84869 entries, 0 to 84868\n",
      "Data columns (total 9 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   GROUP                84869 non-null  object \n",
      " 1   CODE                 84869 non-null  object \n",
      " 2   NAME                 84869 non-null  object \n",
      " 3   YEAR                 84869 non-null  float64\n",
      " 4   DISEASE              84869 non-null  object \n",
      " 5   DISEASE_DESCRIPTION  84869 non-null  object \n",
      " 6   CASES                84869 non-null  float64\n",
      " 7   CASE_SEVERITY        84869 non-null  object \n",
      " 8   RECENT_DATA          84869 non-null  object \n",
      "dtypes: float64(2), object(7)\n",
      "memory usage: 6.5+ MB\n",
      "None\n",
      "               YEAR         CASES\n",
      "count  84869.000000  8.486900e+04\n",
      "mean    2004.108261  3.450124e+03\n",
      "std       12.591396  5.373646e+04\n",
      "min     1980.000000  0.000000e+00\n",
      "25%     1994.000000  0.000000e+00\n",
      "50%     2005.000000  0.000000e+00\n",
      "75%     2015.000000  2.000000e+01\n",
      "max     2023.000000  4.583555e+06\n",
      "\n",
      "üéØ Reported Cases Data Cleaning & Feature Engineering Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 2: Load Dataset\n",
    "# ---------------------------------------------------------\n",
    "data_path = \"../cleaned\"   # Use \"../data\" if original file is there\n",
    "df_cases = pd.read_csv(os.path.join(data_path, \"reported_cases.csv\"))\n",
    "\n",
    "print(\"‚úÖ Reported Cases Dataset Loaded!\")\n",
    "print(df_cases.shape)\n",
    "print(df_cases.head())\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 3: Handle Missing Values\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 3.1 Check missing values before handling\n",
    "print(\"\\nMissing Values Before Handling:\\n\", df_cases.isna().sum())\n",
    "\n",
    "# 3.2 Handle missing CODE and NAME\n",
    "# Drop rows where CODE is missing ‚Äî critical identifier\n",
    "df_cases = df_cases.dropna(subset=['CODE'])\n",
    "\n",
    "# Build CODE ‚Üí NAME mapping from valid entries\n",
    "code_to_name = (\n",
    "    df_cases.dropna(subset=['NAME'])\n",
    "           .drop_duplicates(subset=['CODE'], keep='first')\n",
    "           .set_index('CODE')['NAME']\n",
    "           .to_dict()\n",
    ")\n",
    "\n",
    "# Fill missing NAME using CODE mapping\n",
    "df_cases['NAME'] = df_cases['NAME'].fillna(df_cases['CODE'].map(code_to_name))\n",
    "\n",
    "# Handle World Bank aggregates (if exist)\n",
    "wb_map = {\n",
    "    'WB_LONG_NA': 'World Bank Long-term Aggregate',\n",
    "    'WB_SHORT_NA': 'World Bank Short-term Aggregate'\n",
    "}\n",
    "df_cases.loc[df_cases['CODE'].isin(wb_map.keys()), 'NAME'] = \\\n",
    "    df_cases.loc[df_cases['CODE'].isin(wb_map.keys()), 'CODE'].map(wb_map)\n",
    "\n",
    "# Final fallback: fill missing NAME with \"Unknown\"\n",
    "df_cases['NAME'] = df_cases['NAME'].fillna('Unknown')\n",
    "\n",
    "# 3.3 Handle Missing Disease Description\n",
    "df_cases['DISEASE_DESCRIPTION'] = df_cases['DISEASE_DESCRIPTION'].fillna(df_cases['DISEASE'])\n",
    "\n",
    "# 3.4 Handle Missing CASES\n",
    "# Convert CASES to numeric, replacing errors with NaN\n",
    "df_cases['CASES'] = pd.to_numeric(df_cases['CASES'], errors='coerce')\n",
    "\n",
    "# Fill missing CASES with 0 ‚Äî assuming no reported cases\n",
    "df_cases['CASES'] = df_cases['CASES'].fillna(0)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 4: Normalize Units\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Ensure CASES ‚â• 0\n",
    "df_cases.loc[df_cases['CASES'] < 0, 'CASES'] = 0\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 5: Consistency Checks\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 5.1 Remove duplicates\n",
    "print(f\"\\nTotal Duplicate Rows: {df_cases.duplicated().sum()}\")\n",
    "df_cases.drop_duplicates(inplace=True)\n",
    "\n",
    "# 5.2 Validate year range\n",
    "df_cases = df_cases[(df_cases['YEAR'] >= 1980) & (df_cases['YEAR'] <= 2025)]\n",
    "\n",
    "# 5.3 Standardize disease codes\n",
    "df_cases['DISEASE'] = df_cases['DISEASE'].str.upper().str.strip()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 6: Feature Engineering (FE)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 6.1 Categorize case severity\n",
    "def categorize_cases(cases):\n",
    "    if cases == 0:\n",
    "        return \"No Cases\"\n",
    "    elif cases <= 100:\n",
    "        return \"Low\"\n",
    "    elif cases <= 1000:\n",
    "        return \"Moderate\"\n",
    "    elif cases <= 10000:\n",
    "        return \"High\"\n",
    "    else:\n",
    "        return \"Severe\"\n",
    "\n",
    "df_cases['CASE_SEVERITY'] = df_cases['CASES'].apply(categorize_cases)\n",
    "\n",
    "# 6.2 Add Recent Data flag\n",
    "df_cases['RECENT_DATA'] = df_cases['YEAR'].apply(lambda x: 'Recent' if x >= 2018 else 'Old')\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 7: Final Sanity Check\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n‚úÖ Final Dataset Overview:\\n\")\n",
    "print(df_cases.info())\n",
    "print(df_cases.describe())\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 8: Save Cleaned Dataset\n",
    "# ---------------------------------------------------------\n",
    "cleaned_path = \"../cleaned\"\n",
    "os.makedirs(cleaned_path, exist_ok=True)\n",
    "\n",
    "df_cases.to_csv(os.path.join(cleaned_path, \"reported_cases_cleaned.csv\"), index=False)\n",
    "print(\"\\nüéØ Reported Cases Data Cleaning & Feature Engineering Completed Successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612c6057",
   "metadata": {},
   "source": [
    "## Vaccine Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "443880ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vaccine Introduction Dataset Loaded!\n",
      "(138321, 6)\n",
      "  ISO_3_CODE  COUNTRYNAME WHO_REGION    YEAR                                  DESCRIPTION INTRO\n",
      "0        AFG  Afghanistan       EMRO  2023.0             aP (acellular pertussis) vaccine    No\n",
      "1        AFG  Afghanistan       EMRO  2023.0                          Hepatitis A vaccine    No\n",
      "2        AFG  Afghanistan       EMRO  2023.0                          Hepatitis B vaccine   Yes\n",
      "3        AFG  Afghanistan       EMRO  2023.0                              HepB birth dose   Yes\n",
      "4        AFG  Afghanistan       EMRO  2023.0  Hib (Haemophilus influenzae type B) vaccine   Yes\n",
      "\n",
      "üîç Missing Values BEFORE Handling:\n",
      " ISO_3_CODE     0\n",
      "COUNTRYNAME    1\n",
      "WHO_REGION     1\n",
      "YEAR           1\n",
      "DESCRIPTION    1\n",
      "INTRO          1\n",
      "dtype: int64\n",
      "\n",
      "‚úÖ Missing Values AFTER Handling:\n",
      " ISO_3_CODE     0\n",
      "COUNTRYNAME    0\n",
      "WHO_REGION     0\n",
      "YEAR           0\n",
      "DESCRIPTION    0\n",
      "INTRO          0\n",
      "dtype: int64\n",
      "\n",
      "üßπ Total Duplicate Rows: 0\n",
      "\n",
      "üìä Final Dataset Info:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 119133 entries, 0 to 138320\n",
      "Data columns (total 9 columns):\n",
      " #   Column                     Non-Null Count   Dtype \n",
      "---  ------                     --------------   ----- \n",
      " 0   ISO_3_CODE                 119133 non-null  object\n",
      " 1   COUNTRYNAME                119133 non-null  object\n",
      " 2   WHO_REGION                 119133 non-null  object\n",
      " 3   YEAR                       119133 non-null  int64 \n",
      " 4   DESCRIPTION                119133 non-null  object\n",
      " 5   INTRO                      119133 non-null  object\n",
      " 6   Recently_Introduced        119133 non-null  object\n",
      " 7   Total_Vaccines_Introduced  119133 non-null  int64 \n",
      " 8   Vaccine_Status             119133 non-null  object\n",
      "dtypes: int64(2), object(7)\n",
      "memory usage: 9.1+ MB\n",
      "None\n",
      "\n",
      "üîé Final Missing Values Check:\n",
      " ISO_3_CODE                   0\n",
      "COUNTRYNAME                  0\n",
      "WHO_REGION                   0\n",
      "YEAR                         0\n",
      "DESCRIPTION                  0\n",
      "INTRO                        0\n",
      "Recently_Introduced          0\n",
      "Total_Vaccines_Introduced    0\n",
      "Vaccine_Status               0\n",
      "dtype: int64\n",
      "\n",
      "‚úÖ Vaccine Introduction Data Cleaning & Feature Engineering Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 2: Load Dataset\n",
    "# ---------------------------------------------------------\n",
    "data_path = \"../cleaned\"\n",
    "df_intro = pd.read_csv(os.path.join(data_path, \"vaccine_intro.csv\"))\n",
    "\n",
    "print(\"‚úÖ Vaccine Introduction Dataset Loaded!\")\n",
    "print(df_intro.shape)\n",
    "print(df_intro.head())\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 3: Check Missing Values BEFORE Handling\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\nüîç Missing Values BEFORE Handling:\\n\", df_intro.isna().sum())\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 4: Handle Missing Values\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 4.1 Handle missing ISO_3_Code ‚Üí Drop rows where it's missing since it's a primary identifier\n",
    "df_intro = df_intro.dropna(subset=['ISO_3_CODE'])\n",
    "\n",
    "# 4.2 Handle missing Country Name ‚Üí Map using ISO code if possible\n",
    "code_to_name = (\n",
    "    df_intro.dropna(subset=['COUNTRYNAME'])\n",
    "            .drop_duplicates(subset=['ISO_3_CODE'], keep='first')\n",
    "            .set_index('ISO_3_CODE')['COUNTRYNAME']\n",
    "            .to_dict()\n",
    ")\n",
    "df_intro['COUNTRYNAME'] = df_intro['COUNTRYNAME'].fillna(df_intro['ISO_3_CODE'].map(code_to_name))\n",
    "df_intro['COUNTRYNAME'] = df_intro['COUNTRYNAME'].fillna('Unknown')\n",
    "\n",
    "# 4.3 Handle missing WHO Region ‚Üí Fill missing values with \"Unknown\"\n",
    "df_intro['WHO_REGION'] = df_intro['WHO_REGION'].fillna('Unknown')\n",
    "\n",
    "# 4.4 Handle missing Year ‚Üí Fill with median year (keeps timeline consistent)\n",
    "df_intro['YEAR'] = df_intro['YEAR'].fillna(df_intro['YEAR'].median()).astype(int)\n",
    "\n",
    "# 4.5 Handle missing Description ‚Üí Fill with \"Unknown Vaccine\"\n",
    "df_intro['DESCRIPTION'] = df_intro['DESCRIPTION'].fillna('Unknown Vaccine')\n",
    "\n",
    "# 4.6 Handle missing Intro column ‚Üí Fill with \"No\" (not introduced)\n",
    "df_intro['INTRO'] = df_intro['INTRO'].fillna('No')\n",
    "\n",
    "# ‚úÖ Missing values check AFTER handling\n",
    "print(\"\\n‚úÖ Missing Values AFTER Handling:\\n\", df_intro.isna().sum())\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 5: Data Normalization\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 5.1 Standardize WHO Region text formatting\n",
    "df_intro['WHO_REGION'] = df_intro['WHO_REGION'].str.strip().str.title()\n",
    "\n",
    "# 5.2 Standardize Intro column to Yes/No only\n",
    "df_intro['INTRO'] = df_intro['INTRO'].astype(str).str.strip().str.capitalize()\n",
    "df_intro['INTRO'] = df_intro['INTRO'].replace({'Y': 'Yes', 'N': 'No', 'yes': 'Yes', 'no': 'No'})\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 6: Consistency Checks\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 6.1 Remove duplicates\n",
    "print(f\"\\nüßπ Total Duplicate Rows: {df_intro.duplicated().sum()}\")\n",
    "df_intro.drop_duplicates(inplace=True)\n",
    "\n",
    "# 6.2 Validate Year range ‚Üí Keep only realistic values\n",
    "df_intro = df_intro[(df_intro['YEAR'] >= 1980) & (df_intro['YEAR'] <= 2025)]\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 7: Feature Engineering (FE)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 7.1 Add a flag for \"Recently Introduced\" vaccines\n",
    "df_intro['Recently_Introduced'] = df_intro['YEAR'].apply(lambda x: 'Yes' if x >= 2018 else 'No')\n",
    "\n",
    "# 7.2 Count total vaccines introduced per country (for Power BI insights)\n",
    "df_intro['Total_Vaccines_Introduced'] = df_intro.groupby('COUNTRYNAME')['INTRO'].transform(\n",
    "    lambda x: (x == 'Yes').sum()\n",
    ")\n",
    "\n",
    "# 7.3 Add a Vaccine Status column\n",
    "df_intro['Vaccine_Status'] = df_intro['INTRO'].apply(lambda x: 'Introduced' if x == 'Yes' else 'Not Introduced')\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 8: Final Dataset Overview\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\nüìä Final Dataset Info:\\n\")\n",
    "print(df_intro.info())\n",
    "print(\"\\nüîé Final Missing Values Check:\\n\", df_intro.isna().sum())\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 9: Save Cleaned Dataset\n",
    "# ---------------------------------------------------------\n",
    "cleaned_path = \"../cleaned\"\n",
    "os.makedirs(cleaned_path, exist_ok=True)\n",
    "\n",
    "df_intro.to_csv(os.path.join(cleaned_path, \"vaccine_intro_cleaned.csv\"), index=False)\n",
    "print(\"\\n‚úÖ Vaccine Introduction Data Cleaning & Feature Engineering Completed Successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547674a4",
   "metadata": {},
   "source": [
    "### Vaccine Schedule "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "67fc6e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vaccine Schedule Dataset Loaded!\n",
      "(8053, 12)\n",
      "  ISO_3_CODE COUNTRYNAME WHO_REGION    YEAR VACCINECODE               VACCINE_DESCRIPTION  SCHEDULEROUNDS  TARGETPOP TARGETPOP_DESCRIPTION   GEOAREA AGEADMINISTERED SOURCECOMMENT\n",
      "0        ABW       Aruba       AMRO  2023.0  DTAPHIBIPV  DTaP-Hib-IPV (acellular) vaccine             1.0        NaN       General/routine  NATIONAL              M2           NaN\n",
      "1        ABW       Aruba       AMRO  2023.0  DTAPHIBIPV  DTaP-Hib-IPV (acellular) vaccine             2.0        NaN       General/routine  NATIONAL              M4           NaN\n",
      "2        ABW       Aruba       AMRO  2023.0  DTAPHIBIPV  DTaP-Hib-IPV (acellular) vaccine             3.0        NaN       General/routine  NATIONAL              M6           NaN\n",
      "3        ABW       Aruba       AMRO  2023.0  DTAPHIBIPV  DTaP-Hib-IPV (acellular) vaccine             4.0    B_2YL_W       General/routine  NATIONAL             M15           NaN\n",
      "4        ABW       Aruba       AMRO  2023.0     DTAPIPV      DTaP-IPV (acellular) vaccine             5.0  B_CHILD_W       General/routine  NATIONAL              Y4           NaN\n",
      "\n",
      "Normalized Columns: Index(['iso_3_code', 'countryname', 'who_region', 'year', 'vaccinecode', 'vaccine_description', 'schedulerounds', 'targetpop', 'targetpop_description', 'geoarea', 'ageadministered', 'sourcecomment'], dtype='object')\n",
      "\n",
      "Missing Values Before Handling:\n",
      " iso_3_code                  0\n",
      "countryname                 1\n",
      "who_region                  1\n",
      "year                        1\n",
      "vaccinecode                 1\n",
      "vaccine_description         1\n",
      "schedulerounds              1\n",
      "targetpop                4258\n",
      "targetpop_description       1\n",
      "geoarea                    31\n",
      "ageadministered          1046\n",
      "sourcecomment            2914\n",
      "dtype: int64\n",
      "\n",
      "Missing Values After Handling:\n",
      " iso_3_code               0\n",
      "countryname              0\n",
      "who_region               0\n",
      "year                     0\n",
      "vaccinecode              1\n",
      "vaccine_description      0\n",
      "schedulerounds           0\n",
      "targetpop                0\n",
      "targetpop_description    0\n",
      "geoarea                  0\n",
      "ageadministered          0\n",
      "sourcecomment            0\n",
      "dtype: int64\n",
      "\n",
      "‚úÖ Vaccine Schedule Data Cleaning Completed & Saved Successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 1 ‚Äî Load the dataset\n",
    "# ---------------------------------------------------------\n",
    "data_path = \"../cleaned\"    # change if your dataset is in a different folder\n",
    "df_schedule = pd.read_csv(os.path.join(data_path, \"vaccine_schedule.csv\"))\n",
    "\n",
    "print(\"‚úÖ Vaccine Schedule Dataset Loaded!\")\n",
    "print(df_schedule.shape)\n",
    "print(df_schedule.head())\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 2 ‚Äî Normalize column names\n",
    "# ---------------------------------------------------------\n",
    "df_schedule.columns = df_schedule.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "print(\"\\nNormalized Columns:\", df_schedule.columns)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 3 ‚Äî Check Missing Values Before Handling\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\nMissing Values Before Handling:\\n\", df_schedule.isna().sum())\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 4 ‚Äî Handle Missing Values (NO DROPS)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 4.1 Handle missing ISO_3_Code ‚Üí Fill with \"UNKNOWN\"\n",
    "df_schedule['iso_3_code'] = df_schedule['iso_3_code'].fillna(\"UNKNOWN\")\n",
    "\n",
    "# 4.2 Handle missing Country Name ‚Üí Try mapping from ISO_3_Code, else fill with \"Unknown Country\"\n",
    "code_to_name = (\n",
    "    df_schedule[['iso_3_code', 'countryname']]\n",
    "    .dropna()\n",
    "    .drop_duplicates()\n",
    "    .set_index('iso_3_code')['countryname']\n",
    "    .to_dict()\n",
    ")\n",
    "df_schedule['countryname'] = df_schedule['countryname'].fillna(\n",
    "    df_schedule['iso_3_code'].map(code_to_name)\n",
    ")\n",
    "df_schedule['countryname'] = df_schedule['countryname'].fillna(\"Unknown Country\")\n",
    "\n",
    "# 4.3 Handle missing WHO Region ‚Üí Fill with \"Unknown Region\"\n",
    "df_schedule['who_region'] = df_schedule['who_region'].fillna(\"Unknown Region\")\n",
    "\n",
    "# 4.4 Handle missing Vaccine Description ‚Üí Map from vaccine_code if possible, else \"Unknown Vaccine\"\n",
    "code_to_vaccine = (\n",
    "    df_schedule[['vaccinecode', 'vaccine_description']]\n",
    "    .dropna()\n",
    "    .drop_duplicates()\n",
    "    .set_index('vaccinecode')['vaccine_description']\n",
    "    .to_dict()\n",
    ")\n",
    "df_schedule['vaccine_description'] = df_schedule['vaccine_description'].fillna(\n",
    "    df_schedule['vaccinecode'].map(code_to_vaccine)\n",
    ")\n",
    "df_schedule['vaccine_description'] = df_schedule['vaccine_description'].fillna(\"Unknown Vaccine\")\n",
    "\n",
    "# 4.5 Handle missing Schedule Rounds ‚Üí Fill with \"Not Specified\"\n",
    "df_schedule['schedulerounds'] = df_schedule['schedulerounds'].fillna(\"Not Specified\")\n",
    "\n",
    "# 4.6 Handle missing Target Population ‚Üí Fill with median per vaccine, else 0\n",
    "df_schedule['targetpop'] = df_schedule['targetpop'].fillna(\"Not Specified\")\n",
    "\n",
    "# 4.7 Handle missing Target Population Description ‚Üí Fill with \"General Population\"\n",
    "df_schedule['targetpop_description'] = df_schedule['targetpop_description'].fillna(\"General Population\")\n",
    "\n",
    "# 4.8 Handle missing Geoarea, Age Administered, Source Comment ‚Üí Fill with \"Unknown\"\n",
    "for col in ['geoarea', 'age_administered', 'source_comment']:\n",
    "    if col in df_schedule.columns:\n",
    "        df_schedule[col] = df_schedule[col].fillna(\"Unknown\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 5 ‚Äî Consistency Checks (Fix, Do Not Drop)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 5.1 Fix negative target populations ‚Üí Set to median per vaccine\n",
    "df_schedule['targetpop'] = df_schedule['targetpop'].astype(str).str.upper()\n",
    "\n",
    "\n",
    "# 5.2 Fix invalid years ‚Üí Replace out-of-range values with median year\n",
    "median_year = df_schedule['year'].median()\n",
    "df_schedule['year'] = df_schedule['year'].apply(\n",
    "    lambda y: median_year if pd.notna(y) and (y < 1980 or y > 2030) else y\n",
    ")\n",
    "df_schedule['year'] = df_schedule['year'].fillna(median_year)\n",
    "\n",
    "\n",
    "df_schedule['ageadministered'] = df_schedule['ageadministered'].fillna(\"Not Specified\")\n",
    "\n",
    "# 2.12 Source comment ‚Üí Fill missing with \"Not Provided\"\n",
    "df_schedule['sourcecomment'] = df_schedule['sourcecomment'].fillna(\"Not Provided\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 6 ‚Äî Check Missing Values After Handling\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\nMissing Values After Handling:\\n\", df_schedule.isna().sum())\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 7 ‚Äî Save Cleaned Dataset\n",
    "# ---------------------------------------------------------\n",
    "cleaned_path = \"../cleaned\"\n",
    "os.makedirs(cleaned_path, exist_ok=True)\n",
    "\n",
    "df_schedule.to_csv(os.path.join(cleaned_path, \"vaccine_schedule_cleaned.csv\"), index=False)\n",
    "\n",
    "print(\"\\n‚úÖ Vaccine Schedule Data Cleaning Completed & Saved Successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b00786",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
